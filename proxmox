- Proxmox-VE-Datasheet.pdf

Path to upload templates :: /var/lib/vz/template/cache
Path where ISOs resides :: /var/lib/vz/template/iso
Path where Raw Images resides :: /var/lib/vz/images/
Path cat /etc/pve/qemu-server/108.

:: UPLOAD ISO IMAGES WITH SCP ::

- scp -v xss_and_mysql_file_i386.iso root@10.0.0.200:/var/lib/vz/template/iso
- scp -v * root@10.0.0.200:/var/lib/vz/template/iso

- pveversion -v :: to see the Proxmox Version

[OVERVIEW]

Proxmox VE is a complete virtualization management solution for servers. You can virtualize even the most demanding application
workloads running on Linux and Windows Servers. It combines the leading Kernel-based Virtual Machine (KVM) hypervisor and containerbased virtualization on one management platform. Thanks to the unique multi-master design there is no need for an additional management server. This saves ressources and also allows high availabilty without single point of failures (no SPOF).
With the included web-based management you can easily control all functionality. Full access to all logs from all nodes in a cluster is included, including task logs like running backup/restore processes, live-migration
or high availability (HA) triggered activities.

[ENTERPRISE-READY]

Proxmox VE includes all the functionality you need to deploy an enterprise-class virtualization environment in your company‘s
datacenter. Multiple authentication sources combined with role based user- and permission management enable full control of your HA
virtualization cluster. The RESTful web API enables easy integration for third party management tools like custom hosting environments. With the future-proof open source development model, your full access to the source code as well as maximum flexibility and security are guaranteed.

[Industry-leading Enterprise Virtualization Technology]

- Linux and Windows Servers, 32 and 64 bit operation systems.
- Support for the latest Intel and AMD server chipsets for great VM performance.
- Leading performance relative to bare metal for realworld enterprise workloads.
- Management layer contains all the capabilities required to create and manage a virtual infrastructure.

[Open Source Software]

- Licensed under the free, copyleft GNU Affero General
Public License, version 3 (AGPL, V3: http://www.gnu.org/licenses/agpl-3.0.html)
- Designed to ensure cooperation with community
- Public code repository (GIT)
- Bugtracker
- Public community forum
- Free Wiki for documention and HowTo´s

[Integrated web-based Management GUI]

- No need to install a separate management tool or any additional management node
- Fast search-driven interface, capable of handling thousands of VM´s
- Secure HTML5 VNC console, supporting SSL
- Wizard based creation of virtual servers and containers
- Seamless integration and management of Proxmox VE 4.x Cluster
- Subscription management via GUI
- Role based permission management for all objects (VM´s and CT´s, storages, etc.)
- Support for multiple authentication sources (e.g. local, MS ADS, LDAP, ...)
- AJAX technologies for dynamic updates of resources
- Based on Ext JS JavaScript framework.
- Cluster-wide Task and Cluster logs: The GUI shows all running tasks from the whole cluster but also the history and the syslog of each node. This includes running backup or restore jobs, live-migration or HA triggered activities

[RESTful web API]

- Easy integration for third party management tools like custom hosting enviroments
- REST like API (JSON as primary data format, and the whole API is formally defined using JSON Schema)
- Easy and human readable data format (native web browser format)
- Automatic parameter verification (verification of return values)
- Automatic generation of API documentation
- Easy way to create command line tools (use the same API)
- Resource Oriented Architecture (ROA)
- Declarative API definition using JSON Schema

[High Availability Cluster]

- No single point of failure (no SPOF)
- Mulit-master cluster (no single master)
- GUI for managing KVM and container HA settings
- pmxcfs—Proxmox VE Cluster File System: databasedriven file system for storing configuration files replicated in realtime on all nodes using Corosync
- Based on proven Linux HA technologies, providing stable and reliable HA service
- Resource agents for KVM and Linux Containers (LXC)
- Watchdog based Fencing

[Live Migration]

- Moving QEMU virtual servers from one physical host to another without any downtime.

[Fencing]

- Proxmox VE HA Manger uses self fencing provided by hardware Watchdog or kernel Softdog
- No simultaneous data access and corruption
- Works „out-of-the-box“
- Proxmox VE HA Simulator included for testing

[Command Line Interface]

- For advanced users
- Manage all components of your virtual environment
- CLI with intelligent tab completion and full UNIX man page documentation

[Storage Types]

- Local storage, ZFS, LVM with ext3/ext4, and XFS
- Shared storage such as FC, iSCSI or NFS
- Distributed storage such as Ceph RBD, Sheepdog, GlusterFS and DRBD9
- Unlimited number of storage definitions (cluster-wide)

[Backup and Restore]

- Full backups of containers and VMs
- Live Snapshot Backups
- Multiple schedules and backup storages
- GUI integrations, but also via CLI
- "Backup Now" and restore via GUI
- All jobs from all nodes can be monitored via the GUI tab “Tasks”

[Bridged Networking]

- Bridged networking model
- Each host with up to 4094 bridges
- TCP/IP configuration
- IPv4 and IPv6 support
- VLANs
- Open vSwitch

[Multiple Authentication Sources]

- Proxmox VE supports multiple authentication sources
- Linux PAM standard authentication (e.g. 'root' and other local users)
- Proxmox VE authentication server (built-in)
- Microsoft Active Directory (MS ADS)
- LDAP

[Role-based Administration]

- User- and permission management for all objects (VM´s, storages, nodes, etc.)
- A role is simply a list of privileges. Proxmox VE comes with a number of predefined roles which satisfies most needs. The whole set of predefined roles can be seen on the GUI.
- Permissions are the way to control access to objects. In technical terms they are simply a triple containing <path,user,role>. This concept is also known as access control lists. Each permission specifies a subject (user or group) and a role (set of privileges) on a specific path. 

[VM Templates and Clones]

- Deploying virtual machines from templates is blazing fast, very comfortable and if you use linked clones you can optimize your storage by using base images and thin-provisioning.
- Linked and Full Clones

[Two-Factor Authentication]

- For high security
- 2 types: Time-based One Time Passwords and YubiKey

[Proxmox VE Firewall]

- Linux-based netfilter technology. Stateful firewall. Provides high bandwith.
- Distributed: Main configuration in Proxmox VE cluster file system, iptable rules stored in nodes.
- Supports IPv4 and IPv6
- Cluster-wide settings
- 3 levels of configuration (datacenter, host, VM/CT)
- Completely customizable allowing complex configurations via GUI or CLI
- Quick setup with predefined macros

[Create the Cluster] - [https://pve.proxmox.com/wiki/Proxmox_VE_4.x_Cluster]

Proxmox VE 4.x (and all versions above) cluster enables central management of multiple physical servers. A Proxmox VE Cluster consists of several nodes (up to 32 physical nodes, probably more, dependent on network latency).

[Main features]

- Centralized web management, including secure console
- Support for multiple authentication sources (e.g. local, MS ADS, LDAP, ...)
- Role based permission management for all objects (VM´s, storages, nodes, etc.)
- Creates multi-master clusters
- Proxmox Cluster file system (pmxcfs): Database-driven file system for storing configuration files, replicated in real-time on all nodes using corosync (maximal size 30 MB)
- Migration of Virtual Machines between physical hosts
- Cluster-wide logging
- RESTful web API
- Self-fencing as out of the box method (also possible to use power- or network-fencing).
- Fast deployment
- Cluster-wide Firewall
- Linux Container migration

NOTE: 

- It is not possible to mix Proxmox VE 3.x and earlier with Proxmox VE 4.0 cluster
- All nodes must be in the same network as corosync uses IP Multicast to communicate between nodes 
- Some switches do not support IP multicast by default and must be manually enabled first.
- Date and time have to be synchronized.
- SSH tunnel on port 22 between nodes is used.
- If you are interested in High Availability too, for reliable quorum you must have at least 3 active nodes at all times
- If shared storage is used a dedicated NIC for the traffic is needed.

[Commands]

- pvecm create <cluster_name>
- pvecm status    :: To see the status of a cluster
- pvecm add [ip_server_target]      :: Adding node to the Cluster.
- pveversion
- pveversion -v
- pvecm nodes     					:: Display nodes of the Cluster.
- pvecm delnode hp2					:: Removes a cluster Node.
- corosync-cmapctl -g totem.interface.0.mcastaddr
- omping srv1 srv2 srv3
- clustat - Cluster Status Utility
- clusvcadm - Cluster User Service Administration Utility
- ccs_config_validate - validate cluster.conf file
- fence_tool - a utility for the fenced daemon
- fence_node - a utility to run fence agents
- fence_ack_manual - a utility to manually manage fencing using /bin/false

- systemctl status corosync.service
- corosync-cmapctl 
- corosync-quorumtool


-----------
 Templates
-----------

A template is a fully pre-configured operating system image that can used to deploy KVM virtual machines.

Deploying virtual machines from templates is blazing fast, very comfortable and if you use linked clones you can optimize your storage by using base images and thin-provisioning.

Proxmox VE includes container based templates since 2008 and beginning with the V3.x series, additionally KVM templates can be created and deployed. 

- VM - KVM based virtual machine
- Templates - Templates are pre-configured operating system environments that deploy in a couple of clicks
- Linked Clone - A linked clone VM requires less disk space but cannot run without access to the base VM Template
- Full Clone - A full clone VM is a complete copy and is fully independant to the original VM or VM Template, but requires the same disk space as the original. 

[ Create VM Template ]

Templates are created by converting a VM to a template.

 - Install your VM with all drivers and needed software packages
 - Remove all user data, passwords and keys - In order to do this run sysprep on Windows or similar tools or scripts on Linux and just power-off the VM.
 - Right-click the VM and select "Convert to template" 

As soon as the VM is converted, it cannot be started anymore and the icon changes. If you want to modify an existing template, you need to deploy a full clone from this template and do the steps above again. 

--------------
 Cache Types
--------------
cache=none seems to be the best performance and is the default since Proxmox 2.X.

- Standard
- Directsync
- Write through
- Write back
- Write back (insecure)
- None

-------------
 Bus/Device
-------------
- VIRTIO
- 

--------
 Format
--------
- QEMU
-
-
------------------
 USB Installation
------------------

- dd if=proxmox-ve-4.1-e1f08ccd-6.iso of=/dev/sdb[usb_target] bs=1M  :: Copy the proxmox iso image to a flash usb drive.
- Suse studio imagewritter

--------------------
 Host Configuration
--------------------

- /etc/hosts
	- 10.0.0.200 pve-server01.dominio.local pve-server01

- nano /etc/apt/sources.list
	- deb http://ftp.at.debian.org/ wheezy main contrib
	- deb http://download.proxmox/debian wheezy pve-no-subscription
	- deb http://security.debian.org/ wheezy/updates main contrib

- wget -O- "http://download.proxmox.com/debian/keys.asc" |apt-key add -

- apt-get update && apt-get dist-upgrade

- apt-get install pve-firmware pve-kernel-2.6.32-24-pve

- apt-get install pve-headers-2.6.32-24-pve

- apt-get remove linux-image-amd64 linux-image-3.2.0-4-amd64   :: To remove the SO Kernel.

- update-grub

- apt-get install proxmox-ve-2.6.32 ntp ssh lvm2 postfix ksm-control-daemon vzprocps open-iscsi bootlogd


===============================
 HTTPSCertificateConfiguration
===============================

- mkdir /etc/pve/.le
- cd /root/acme-master
- ./acme.sh --install --accountconf /etc/pve/.le/account.conf --accountkey /etc/pve/.le/account.key --accountemail "ksanchez@myemail.com"

Check the config file in /etc/pve/.le/account.conf and verify:

the ACCOUNT_EMAIL variable should be set to your email address
the ACCOUNT_KEY_PATH variable should be set to "/etc/pve/.le/account.key"
You can edit this file with your favourite text editor if either of those is incorrect.

- acme.sh --issue --standalone --keypath /etc/pve/local/pveproxy-ssl.key --fullchainpath /etc/pve/local/pveproxy-ssl.pem --reloadcmd "systemctl restart pveproxy" -d colorapp.local

- systemctl restart pveproxy  :: Restart the Web Interface

----------
 PVE Logs
----------

- journalctl -b -u pveproxy.service

===============
 Storage Model
===============

-> https://pve.proxmox.com/wiki/Storage_Model

Proxmox VE is using a very flexible storage model. Virtual machine images can be stored on local storage (and more than one local storage type is supported) as well as on shared storage like NFS and on SAN.

You may configure as many storage definitions as you like!

One major benefit of storing VMs on shared storage is the ability to live-migrate running machines without any downtime, as all nodes in the cluster have direct access to VM disk images.

Note: OpenVZ containers must be on local storage or NFS.

The following storage types can be added via the web interface.

--------------------------------
Network storage types supported
--------------------------------

- LVM Group (network backing with iSCSI targets)
- iSCSI target
- NFS Share
- Direct to iSCSI LUN
- GLusterFS
- RBD/Ceph
- Sheepdog (still not stable, so please do not use it in production environments.)

--------------------------------
 Local storage types supported
--------------------------------

- LVM Group (local backing devices like block devices, FC devices, DRBD, etc.)
- Directory (storage on existing filesystem)
- ZFS local pool (beta state, available since Proxmox VE 3.4)

some storage types have only one possible use, which is available as soon as you define the storage entry, you don't need to specify, ie:

- iSCSI: this storage is either used directly (Note: this can be dangerous, see further down), or as a base to define local LVM storage entries (see further down), or as ZFS, it needs a portal address and a target ID

- LVM: this storage type is only used to store RAW VM disk images, and first needs an iSCSI "base storage" entry defined or local disks prepared as LVM storage.

- ZFS over iSCSI: this storage type is only used to store RAW VM disk images, and first needs an iSCSI "base storage" entry defined. (available since Proxmox VE 3.2)

- ZFS: this storage type is used to store RAW VM disk images, but can also be used as a directory. (available since Proxmox VE 3.4)

- RBD/Ceph: this storage type is only used to store RAW VM disk images, and needs a working "Monitor host"

- Sheepdog: this storage type is only used to store RAW VM disk images, and needs a working "portal" (still not stable, so please do not use it in production environments.)

- Images: to store working VM (kvm) disk images
this will create a ./images folder on the filesystem
./images will have a folder for each VM containing disks on this storage - used/unused disks in file formats qcow2, vmdk, RAW. etc and also qcow2 snapshots diff files if created - named by its VMID

- ISO: to store .iso files to use in VM virtual cd drives
this will create a ./template/iso folder on the filesystem
./template/iso will contain all ".iso" files uploaded to this storage, available in the web gui for the VMs´ ISO selection for virtual cd/dvd drives. Not available for CTs.

- Templates: to store CT (openvz) templates
this will create a ./template/cache folder on the filesystem
./template/cache will contain all ".tar.gz" files uploaded to this storage, available in the web gui to create a new CT from such templates.

- Backups: to store VM/CT backups
this will create a ./dump folder on the filesystem
./dump will contain all backup files (currently .vma.lzo or .vma.gz depending on backup settings) and, for each backup file, a related .log file.

- Containers: to store working CT (openvz) disk images (file and folders directly on the storage filesystem)
this will create a ./private folder on the filesystem,
./private will contain a folder for each CT defined on this storage, named by its VMID, containing the whole local CT filesystem.

[SSL CERTIFICATES]

:: Let's Encrypt using acme.sh :: 

-  pveversion
- git clone https://github.com/Neilpang/acme.sh.git
- mkdir /etc/pve/.le
- cd /root/acme-master
- ./acme.sh --install --accountconf /etc/pve/.le/account.conf --accountkey /etc/pve/.le/account.key --accountemail "$EMAIL"
- Check the config file in /etc/pve/.le/account.conf and verify:
	- the ACCOUNT_EMAIL variable should be set to your email address
	- the ACCOUNT_KEY_PATH variable should be set to "/etc/pve/.le/account.key"
- journalctl -b -u pveproxy.service

-> https://pve.proxmox.com/wiki/HTTPSCertificateConfiguration


::: force remove a node from proxmox node tree :::

- rm -rf /etc/pve/nodes/HOSTNAME

::: How to reset cluster configuration in Proxmox ::: 

-> https://elkano.org/blog/how-to-reset-cluster-configuration-in-proxmox-2/

- cp -a /etc/pve /root/pve_backup
- /etc/init.d/pve-cluster stop
- umount /etc/pve
- /etc/init.d/cman stop 
- rm /etc/cluster/cluster.conf
- rm -rf /var/lib/pve-cluster/*
- /etc/init.d/pve-cluster start
- pvecm create newcluster 
- cp /root/pve_backup/*.cfg /etc/pve/
- cp /root/pve_backup/qemu-server/*.conf /etc/pve/qemu-server/
- cp /root/pve_backup/openvz/* /etc/pve/openvz/
- rm -rf /etc/pve/nodes/HOSTNAME

================
 Network Model
================

Proxmox VE uses a bridged networking model. Each host can have up to 4094 bridges. Bridges are like physical network switches implemented in software on the Proxmox VE host. All VMs can share one bridge as if virtual network cables from each guest were all plugged into the same switch. For connecting VMs to the outside world, bridges are attached to physical network cards assigned a TCP/IP configuration. For further flexibility, VLANs (IEEE 802.1q) and network bonding/aggregation are possible. In this way it is possible to build complex, flexible virtual networks.

Changes are stored to /etc/network/interfaces.new, and are activated when you reboot the host. Actual configuration resides in /etc/network/interfaces.

The installation program creates a single bridge (vmbr0), which is connected to the first ethernet card (eth0).

-> https://pve.proxmox.com/wiki/Network_Model

====================
 Trainings Content
====================

The training course is comprised of several modules and practical lab exercises, covering deployment, setup, and configuration. The combination of theory and practice helps trainees build their knowledge and easily apply the concepts being discussed.

Overview about Proxmox VE
Concepts / Architecture / Technology (KVM and container)
Requirements (Hardware, Storage, Network)
Installation and Software updates
Introduction to Web-based Management
Network Model
Storage Model
Create and manage Virtual Machines (Windows & Linux)
Create and manage Containers (Linux only)
Manage VM/CT Startup and Shutdown behavior
Backup and Restore, Scheduling
Proxmox VE Cluster - Setup, configuration, management (basic and advanced)
Live Migration
Proxmox VE Firewall
Authentication realms and user management
CLI Tools
VMID.conf
pmxcfs - how to use the unique Proxmox database-driven file system for storing configuration files
Troubleshooting
High Availability with Proxmox VE HA Manager, and Fencing
Advanced Storage
Upcoming Features (Roadmap)
Support offerings
Practice

[Proxmox VE Advanced]

The training course is comprised of several modules and practical lab exercises, covering deployment, setup, and advanced configuration. The combination of theory and practice helps trainees build their knowledge and easily apply the concepts being discussed.

Proxmox VE Cluster - Advanced setup, configuration, management
Proxmox VE HA Manger and corosync
pmxcfs - how to use the unique Proxmox database-driven file system for storing configuration files
Authentication realms and user management - Advanced setup, configuration, management
Two-Factor Authentication
Troubleshooting
Proxmox VE API
High Availability and Fencing
Advanced storage - distributed storage
Upcoming Features (Roadmap)
Support offerings
Practice

[Clases Espanol]

 Clase 1 >> Introducción a la virtualización, modelos de virtualización y almacenamiento virtualizado Página
 Clase 2 >> Instalación de Proxmox e inicios con OpenVZ
 Clase 3 >> Gestión de Conjuntos, usuarios y permisos en Proxmox. Comandos y tips de OpenVZ, Inicio de KVM
 Clase 4 >> Comandos KVM, instalación desde Debian y Backups
 Clase 5 >> Backups, formato de imagenes, clonación / plantillas y creación del cluster
 Clase 6 >> Migración de máquinas virtuales, modos caché y performance de almacenamiento (I) y conceptos de red en virtualización
 Clase 7 >> Conceptos de almacenamiento, tipo de contenidos, almacenamiento tipo directorio, nfs y glusterfs
 Clase 8 >> Almacenamiento distribuido: GlusterFS, iSCSI y DRBD
Introducción a la Virtualización
 Introducción
 Modelos de Virtualización
 Ventajas de Virtualizar
 Hardware recomendado
 Virtualización de almacenamiento
 Virtualización con Software Libre
Tipos de Virtualización en Proxmox
 Containers
 Openvz
 Máquinas Virtuales
 Kvm
 Diferencias entre Openvz y Kvm
 Infraestructuras virtuales
Virtualización con Proxmox
 Instalación de Proxmox 2
 Instalación 'from scratch'
 Instalación en un ISP
 Securizando la instalación
Trabajando con Kvm
 Instalación de una Máquina Virtual
 Fuente de Instalación
 Tipos de almacenamiento
 CPU y Memoria
 Configuración de la red
Operaciones Avanzadas con Kvm
 Tipos de conexiones de Red
 Tuneando KVM
 Migrando a KVM
 Passthrough de dispositivos
 Duplicación de Maquinas Virtuales
Trabajando con Openvz
 Instalación de un Container
 Configuraciones básicas
 Gestionando recursos
 Ventajas de virtualización en containers
Operaciones Avanzadas con Openvz
 Trabajando con templates
 Creando un template
 Comandos para administración en consola
 Controlando recursos desde Consola
 Modificando los recursos desde Consola
Casos prácticos
 Crear y configurar un Cluster Proxmox
 Crear y configurar DRBD en Proxmox
 Crear y configurar NFS en Proxmox
 Crear y configurar Glusterfs en Proxmox
Anexo: Conceptos teóricos de gnu/linux
 Diferencias entre un usuario normal y el administrador
 Editor de texto Vi
 Tareas programadas

 [Appliance Downloads]

 - http://download.proxmox.com/appliances/system/
 - http://openvztemplates.com/
 - http://download.openvz.org/template/precreated/contrib/
 - http://download.proxmox.com/images/system/

[INFO]

- https://www.proxmox.com/en/training/pve-bundle
- https://www.proxmox.com/en/training/pve-advanced
- https://openwebinars.net/cursos/curso-online-de-virtualizacion-de-servidores/?ref=landing-cursos-box
- https://www.youtube.com/watch?v=2qjtpwuMFuI
- Web: http://www.proxmox.com
- Wiki Proxmox VE: http://pve.proxmox.com/wiki
- Roadmap: http://pve.proxmox.com/wiki/Roadmap
- Downloads: http://www.proxmox.com/downloads
- https://plus.google.com/+proxmox
- http://www.proxmox.com/en/downloads
- http://pve.proxmox.com/wiki/High_Availability_Cluster
- http://pve.proxmox.com/wiki/High_Availability_Cluster#Certified_Configurations_and_Examples
- https://pve.proxmox.com/wiki/Performance_Tweaks
